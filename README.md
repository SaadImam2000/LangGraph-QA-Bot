# LangGraph-QA-Bot
An enterprise-grade AI assistant built with LangGraph, featuring RAG (Retrieval Augmented Generation), intelligent tool routing, conversation memory, caching, rate limiting, and production-ready optimizations.

âœ¨ Features
Core Capabilities

ğŸ§® Calculator Tool - Safe mathematical expression evaluation
ğŸ“š RAG System - Document Q&A with confidence scoring and source attribution
ğŸ“„ File Reader - Read text files with security validation
ğŸ” Memory Search - Intelligent conversation history search with relevance ranking
ğŸ’¬ Natural Chat - Context-aware conversational AI

Production Features

âš¡ Response Caching - LRU cache with TTL for faster responses
ğŸ›¡ï¸ Rate Limiting - Prevent abuse with configurable limits
ğŸ“Š Metrics Tracking - Monitor performance and usage patterns
ğŸ¯ Intent Detection - Smart routing to appropriate tools
ğŸ”„ Error Recovery - Robust error handling with retry logic
ğŸ“ˆ Confidence Scoring - Know when to trust RAG results

ğŸš€ Quick Start
Installation
bash# 
Clone repository
git clone https://github.com/SaadImam2000/langgraph-qa-bot.git
cd langgraph-qa-bot

# Install dependencies
pip install -r requirements.txt

# Run the application
langgraph_bot.py

Google Colab

!pip install -r requirements.txt
!python langgraph_bot.py

ğŸ“‹ Requirements

Python 3.9+
8GB+ RAM (16GB recommended)
GPU optional (CPU mode works but slower)

ğŸ¯ Usage Examples

Calculator
User: Calculate (155 * 8 + 42) / 7
Bot: âœ… Result: 183.142857

Document Q&A
User: What are the company's work hours?
Bot: Regular hours are 9:00 AM to 5:00 PM, Monday through Friday...
     ğŸŸ¢ Sources: sample_policy.txt

Memory Search
User: What did we discuss about benefits?
Bot: ğŸ” Found 3 matching messages:
     Message 2/10 - You: Tell me about benefits

âš™ï¸ Configuration
Environment Variables
bash# Optional: Disable Gradio analytics
export GRADIO_ANALYTICS_ENABLED=False

# Optional: Custom cache settings
export CACHE_MAX_SIZE=100
export CACHE_TTL_SECONDS=3600

ğŸ“Š Performance

Average Latency: <100ms (cached), 2-5s (uncached)
Cache Hit Rate: 60-80% in typical usage
Throughput: 10+ queries/second
Memory: ~4GB with Phi-3 model

ğŸ”§ Troubleshooting
Out of Memory

Reduce model size: Use google/flan-t5-base instead of Phi-3
Decrease chunk size in RAG: chunk_size=400
Clear cache frequently: Click "Clear Cache" button

Slow Responses

Enable response caching (default)
Use smaller embedding model
Reduce max_steps: max_steps=10

PDF Upload Issues

Check file size (<10MB)
Ensure text-based PDF (not scanned images)
Use PDF/A format when possible

ğŸ¤ Contributing
Contributions welcome! Please:

Fork the repository
Create feature branch (git checkout -b feature/amazing-feature)
Commit changes (git commit -m 'Add amazing feature')
Push to branch (git push origin feature/amazing-feature)
Open Pull Request

See CONTRIBUTING.md for detailed guidelines.

ğŸ“„ License
This project is licensed under the MIT License - see LICENSE file.

ğŸ™ Acknowledgments

Built with LangChain and LangGraph
Uses Gradio for web interface
Powered by Hugging Face Transformers

Star â­ this repo if you find it useful!
