# -*- coding: utf-8 -*-
"""QA-Bot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11XYm6Pn_4xk9x0HnbiyiEshnlNZlrwSc

**Hugging Face Login**
"""

from huggingface_hub import login
login()  # Optional: for gated models

"""**Installation**"""

!pip install -q -r requirements.txt

print("âœ… Packages installed!")
print("âš ï¸  IMPORTANT: Click 'Runtime â†’ Restart Runtime' now!")
print("âš ï¸  After restart, run the main code cell")

"""**Disable Gradio analytics**"""

import os
os.environ['GRADIO_ANALYTICS_ENABLED'] = 'False'

"""**Imports**"""

import gradio as gr
import re
import asyncio
import time
import hashlib
from typing import List, Optional, Literal, Dict, Any, Tuple
from datetime import datetime
from dataclasses import dataclass, field
from enum import Enum
from functools import wraps
from collections import defaultdict
import logging
from pathlib import Path

from pydantic import BaseModel, Field, field_validator, ConfigDict
from transformers import pipeline
from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_community.chat_message_histories import ChatMessageHistory
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

"""**LOGGING CONFIGURATION**"""

class ColoredFormatter(logging.Formatter):
    """Custom formatter with colors for better readability."""

    COLORS = {
        'DEBUG': '\033[36m',    # Cyan
        'INFO': '\033[32m',     # Green
        'WARNING': '\033[33m',  # Yellow
        'ERROR': '\033[31m',    # Red
        'CRITICAL': '\033[35m', # Magenta
        'RESET': '\033[0m'
    }

    def format(self, record):
        color = self.COLORS.get(record.levelname, self.COLORS['RESET'])
        record.levelname = f"{color}{record.levelname}{self.COLORS['RESET']}"
        return super().format(record)

def setup_logging(level=logging.INFO):
    """Configure structured logging."""
    logger = logging.getLogger('langgraph_bot')
    logger.setLevel(level)

    handler = logging.StreamHandler()
    formatter = ColoredFormatter(
        '%(asctime)s | %(levelname)s | %(name)s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)

    return logger

logger = setup_logging()

"""**METRICS TRACKING**"""

@dataclass
class Metrics:
    """Track system performance metrics."""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    total_latency: float = 0.0
    cache_hits: int = 0
    cache_misses: int = 0
    tool_calls: Dict[str, int] = field(default_factory=lambda: defaultdict(int))
    rag_queries: int = 0
    errors_by_type: Dict[str, int] = field(default_factory=lambda: defaultdict(int))

    def record_request(self, success: bool, latency: float):
        """Record request metrics."""
        self.total_requests += 1
        if success:
            self.successful_requests += 1
        else:
            self.failed_requests += 1
        self.total_latency += latency

    def get_avg_latency(self) -> float:
        """Calculate average latency."""
        if self.total_requests == 0:
            return 0.0
        return self.total_latency / self.total_requests

    def get_success_rate(self) -> float:
        """Calculate success rate."""
        if self.total_requests == 0:
            return 0.0
        return (self.successful_requests / self.total_requests) * 100

    def get_cache_hit_rate(self) -> float:
        """Calculate cache hit rate."""
        total = self.cache_hits + self.cache_misses
        if total == 0:
            return 0.0
        return (self.cache_hits / total) * 100

    def get_stats(self) -> str:
        """Get formatted metrics."""
        return f"""ðŸ“Š System Metrics:
â€¢ Total Requests: {self.total_requests}
â€¢ Success Rate: {self.get_success_rate():.1f}%
â€¢ Avg Latency: {self.get_avg_latency():.3f}s
â€¢ Cache Hit Rate: {self.get_cache_hit_rate():.1f}%
â€¢ RAG Queries: {self.rag_queries}
â€¢ Tool Usage: {dict(self.tool_calls)}"""

metrics = Metrics()

"""**RESPONSE CACHE**"""

class ResponseCache:
    """LRU cache for responses with TTL."""

    def __init__(self, max_size: int = 100, ttl_seconds: int = 3600):
        self.cache: Dict[str, Tuple[str, float]] = {}
        self.max_size = max_size
        self.ttl = ttl_seconds

    def _hash_key(self, text: str) -> str:
        """Generate cache key."""
        return hashlib.md5(text.encode()).hexdigest()

    def get(self, query: str) -> Optional[str]:
        """Retrieve cached response."""
        key = self._hash_key(query)
        if key in self.cache:
            response, timestamp = self.cache[key]
            if time.time() - timestamp < self.ttl:
                logger.debug(f"Cache HIT for query: {query[:50]}")
                metrics.cache_hits += 1
                return response
            else:
                del self.cache[key]

        metrics.cache_misses += 1
        logger.debug(f"Cache MISS for query: {query[:50]}")
        return None

    def set(self, query: str, response: str):
        """Store response in cache."""
        if len(self.cache) >= self.max_size:
            # Remove oldest entry
            oldest_key = min(self.cache.keys(), key=lambda k: self.cache[k][1])
            del self.cache[oldest_key]

        key = self._hash_key(query)
        self.cache[key] = (response, time.time())
        logger.debug(f"Cached response for query: {query[:50]}")

    def clear(self):
        """Clear cache."""
        self.cache.clear()
        logger.info("Cache cleared")

response_cache = ResponseCache()

"""**RATE LIMITER**"""

class RateLimiter:
    """Simple rate limiter for tool calls."""

    def __init__(self, max_calls: int = 10, window_seconds: int = 60):
        self.max_calls = max_calls
        self.window = window_seconds
        self.calls: Dict[str, List[float]] = defaultdict(list)

    def is_allowed(self, identifier: str) -> bool:
        """Check if request is within rate limit."""
        now = time.time()
        # Remove old calls outside window
        self.calls[identifier] = [
            t for t in self.calls[identifier]
            if now - t < self.window
        ]

        if len(self.calls[identifier]) >= self.max_calls:
            logger.warning(f"Rate limit exceeded for: {identifier}")
            return False

        self.calls[identifier].append(now)
        return True

rate_limiter = RateLimiter()

"""**ENHANCED STATE DEFINITION**"""

class ActionType(str, Enum):
    """Valid action types."""
    USE_TOOL = "use_tool"
    USE_RAG = "use_rag"
    CHAT = "chat"
    RESPOND = "respond"
    ERROR = "error"

class AgentState(BaseModel):
    """Enhanced typed state with validation."""
    messages: List[HumanMessage | AIMessage] = Field(default_factory=list)
    user_input: str = ""
    next_action: str = "chat"  # Simplified to string instead of enum

    # Tool execution
    tool_name: Optional[str] = None
    tool_input: Optional[str] = None
    tool_result: Optional[str] = None

    # RAG
    rag_context: Optional[str] = None
    rag_sources: List[Document] = Field(default_factory=list)
    rag_confidence: float = 0.0

    # Response
    final_answer: Optional[str] = None

    # Error handling & cycle control
    error_count: int = 0
    step_count: int = 0
    max_steps: int = 15  # Increased for complex queries
    error_message: Optional[str] = None

    # Metadata
    start_time: float = Field(default_factory=time.time)
    request_id: str = Field(default_factory=lambda: hashlib.md5(str(time.time()).encode()).hexdigest()[:8])

    @field_validator('step_count')
    @classmethod
    def check_max_steps(cls, v, info):
        """Validate step count."""
        max_steps = info.data.get('max_steps', 15)
        if v > max_steps:
            raise ValueError("Maximum steps exceeded")
        return v

    model_config = ConfigDict(arbitrary_types_allowed=True)

"""**TOOL SYSTEM**"""

class ToolResult(BaseModel):
    """Structured tool result."""
    success: bool
    result: str
    error: Optional[str] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)

def rate_limited(identifier_func):
    """Decorator for rate limiting."""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            identifier = identifier_func(*args, **kwargs)
            if not rate_limiter.is_allowed(identifier):
                return ToolResult(
                    success=False,
                    result="",
                    error="â±ï¸ Rate limit exceeded. Please wait before trying again."
                )
            return func(*args, **kwargs)
        return wrapper
    return decorator

class ImprovedToolRegistry:
    """Enhanced tool registry with better error handling."""

    @staticmethod
    @rate_limited(lambda expr, **kw: "calculator")
    def calculator(expression: str, **kwargs) -> ToolResult:
        """Enhanced calculator with better validation."""
        try:
            # More comprehensive validation
            if not expression or len(expression) > 200:
                return ToolResult(
                    success=False,
                    result="",
                    error="Expression too long or empty"
                )

            # Whitelist validation
            if not re.match(r'^[\d\s+\-*/().]+$', expression):
                return ToolResult(
                    success=False,
                    result="",
                    error="Invalid characters. Use only: 0-9 + - * / ( )"
                )

            # Count parentheses
            if expression.count('(') != expression.count(')'):
                return ToolResult(
                    success=False,
                    result="",
                    error="Mismatched parentheses"
                )

            # Evaluate safely
            result = eval(expression, {"__builtins__": {}}, {})

            # Format result
            if isinstance(result, float):
                result_str = f"{result:.6f}".rstrip('0').rstrip('.')
            else:
                result_str = str(result)

            logger.info(f"Calculator: {expression} = {result_str}")
            return ToolResult(
                success=True,
                result=f"âœ… **Result:** {result_str}",
                metadata={"expression": expression, "value": result}
            )

        except ZeroDivisionError:
            return ToolResult(success=False, result="", error="Division by zero")
        except Exception as e:
            logger.error(f"Calculator error: {e}")
            return ToolResult(success=False, result="", error=f"Calculation error: {str(e)}")

    @staticmethod
    @rate_limited(lambda path, **kw: f"file_reader_{path}")
    def file_reader(filepath: str, **kwargs) -> ToolResult:
        """Enhanced file reader with better security."""
        try:
            path = Path(filepath)

            # Security checks
            if not path.exists():
                return ToolResult(success=False, result="", error=f"File not found: {filepath}")

            if not path.is_file():
                return ToolResult(success=False, result="", error=f"Not a file: {filepath}")

            # Check file extension
            if path.suffix.lower() not in ['.txt', '.md', '.log', '.csv']:
                return ToolResult(success=False, result="", error="Only .txt, .md, .log, .csv files allowed")

            # Check size
            size = path.stat().st_size
            if size > 1_000_000:  # 1MB
                return ToolResult(success=False, result="", error="File too large (max 1MB)")

            # Read file
            content = path.read_text(encoding='utf-8', errors='ignore')

            # Truncate if needed
            preview_length = 2000
            if len(content) > preview_length:
                content = content[:preview_length] + f"\n\n...(showing first {preview_length} characters)"

            logger.info(f"File read: {filepath} ({size} bytes)")
            return ToolResult(
                success=True,
                result=f"ðŸ“„ **Content of {path.name}:**\n\n{content}",
                metadata={"filepath": str(path), "size": size}
            )

        except Exception as e:
            logger.error(f"File reader error: {e}")
            return ToolResult(success=False, result="", error=f"Error reading file: {str(e)}")

    @staticmethod
    def memory_search(query: str, thread_id: str = "default", **kwargs) -> ToolResult:
        """Enhanced memory search with ranking."""
        try:
            history = memory_store.get(thread_id, ChatMessageHistory())
            messages = history.messages

            if not messages:
                return ToolResult(
                    success=True,
                    result="ðŸ’­ No conversation history yet.",
                    metadata={"matches": 0}
                )

            # Enhanced search with scoring
            query_lower = query.lower()
            query_words = set(query_lower.split())

            results = []
            for i, msg in enumerate(messages):
                content_lower = msg.content.lower()

                # Calculate relevance score
                score = 0
                if query_lower in content_lower:
                    score += 10  # Exact phrase match

                # Word overlap
                content_words = set(content_lower.split())
                overlap = len(query_words & content_words)
                score += overlap

                if score > 0:
                    role = "You" if isinstance(msg, HumanMessage) else "Assistant"
                    preview = msg.content[:150] + "..." if len(msg.content) > 150 else msg.content
                    timestamp = f"Message {i+1}/{len(messages)}"
                    results.append((score, f"**{timestamp}** - {role}: {preview}"))

            # Sort by score
            results.sort(reverse=True, key=lambda x: x[0])

            if results:
                top_results = [r[1] for r in results[:5]]
                result_text = f"ðŸ” **Found {len(results)} matching message(s):**\n\n" + "\n\n".join(top_results)
                logger.info(f"Memory search: '{query}' found {len(results)} matches")
                return ToolResult(
                    success=True,
                    result=result_text,
                    metadata={"matches": len(results), "query": query}
                )
            else:
                return ToolResult(
                    success=True,
                    result=f"ðŸ” No messages found containing '{query}'",
                    metadata={"matches": 0}
                )

        except Exception as e:
            logger.error(f"Memory search error: {e}")
            return ToolResult(success=False, result="", error=f"Search error: {str(e)}")

    @staticmethod
    def get_tool_list() -> Dict[str, Dict[str, Any]]:
        """Get available tools with metadata."""
        return {
            "calculator": {
                "function": ImprovedToolRegistry.calculator,
                "description": "Performs mathematical calculations (supports +, -, *, /, parentheses)",
                "keywords": ["calculate", "compute", "solve", "math", "what is"],
                "examples": ["calculate 50*20", "what is 123+456", "(100+50)*2"]
            },
            "file_reader": {
                "function": ImprovedToolRegistry.file_reader,
                "description": "Reads text files (.txt, .md, .log, .csv)",
                "keywords": ["read file", "open file", "show file"],
                "examples": ["read file data.txt", "open file report.md"]
            },
            "memory_search": {
                "function": ImprovedToolRegistry.memory_search,
                "description": "Searches conversation history with relevance ranking",
                "keywords": ["search memory", "what did we", "earlier", "previously"],
                "examples": ["search memory for policy", "what did we discuss earlier"]
            }
        }

"""**RAG SYSTEM**"""

class EnhancedRAGSystem:
    """Improved RAG with reranking and better prompts."""

    def __init__(self):
        self.vectorstore = None
        self.embeddings = None
        self.documents = []
        self.document_metadata = []
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,  # Slightly larger chunks
            chunk_overlap=150,  # More overlap for better context
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]  # Better splitting
        )

    def initialize_embeddings(self) -> bool:
        """Load embeddings with caching."""
        if self.embeddings is None:
            try:
                logger.info("Loading embeddings model...")
                self.embeddings = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/all-MiniLM-L6-v2",
                    model_kwargs={'device': 'cpu'},
                    encode_kwargs={'normalize_embeddings': True},
                    cache_folder=".embedding_cache"  # Cache embeddings
                )
                logger.info("âœ… Embeddings loaded successfully")
                return True
            except Exception as e:
                logger.error(f"Failed to load embeddings: {e}")
                return False
        return True

    def load_pdf(self, pdf_path: str) -> bool:
        """Load PDF with enhanced metadata and image detection."""
        try:
            logger.info(f"Loading PDF: {pdf_path}")

            # Quick file size check
            file_size = os.path.getsize(pdf_path)
            if file_size > 10_000_000:  # 10MB limit
                logger.warning(f"PDF too large: {file_size} bytes")
                return False

            # Load with timeout protection
            loader = PyPDFLoader(pdf_path)
            docs = loader.load()

            # Check if PDF has actual text content
            total_text_length = sum(len(doc.page_content.strip()) for doc in docs)

            if total_text_length < 50:
                # This is likely an image-only PDF
                logger.warning(f"PDF appears to be image-only (only {total_text_length} chars extracted)")
                # Create a metadata-only document
                placeholder_doc = Document(
                    page_content=f"This PDF file ({os.path.basename(pdf_path)}) contains {len(docs)} page(s) but appears to be image-based with no extractable text. OCR functionality is not available in this version.",
                    metadata={
                        'source_file': os.path.basename(pdf_path),
                        'chunk_id': 0,
                        'total_chunks': 1,
                        'loaded_at': datetime.now().isoformat(),
                        'file_type': 'pdf_image_only',
                        'pages': len(docs)
                    }
                )
                self.documents.append(placeholder_doc)
                self.document_metadata.append({
                    'filename': os.path.basename(pdf_path),
                    'pages': len(docs),
                    'chunks': 1,
                    'loaded_at': datetime.now(),
                    'type': 'pdf_image_only',
                    'warning': 'Image-only PDF, no text extracted'
                })
                logger.info(f"âœ… Loaded PDF metadata (image-only): {pdf_path}")
                return True

            # Normal text-based PDF processing
            chunks = self.text_splitter.split_documents(docs)

            # Enhanced metadata
            for i, chunk in enumerate(chunks):
                chunk.metadata.update({
                    'source_file': os.path.basename(pdf_path),
                    'chunk_id': i,
                    'total_chunks': len(chunks),
                    'loaded_at': datetime.now().isoformat(),
                    'file_type': 'pdf'
                })

            self.documents.extend(chunks)
            self.document_metadata.append({
                'filename': os.path.basename(pdf_path),
                'pages': len(docs),
                'chunks': len(chunks),
                'loaded_at': datetime.now(),
                'type': 'pdf'
            })

            logger.info(f"âœ… Loaded PDF: {pdf_path} ({len(docs)} pages, {len(chunks)} chunks)")
            return True

        except Exception as e:
            logger.error(f"Error loading PDF: {e}")
            return False

    def load_text_file(self, text_path: str) -> bool:
        """Load text file with enhanced metadata."""
        try:
            loader = TextLoader(text_path, encoding='utf-8')
            docs = loader.load()
            chunks = self.text_splitter.split_documents(docs)

            # Enhanced metadata
            for i, chunk in enumerate(chunks):
                chunk.metadata.update({
                    'source_file': os.path.basename(text_path),
                    'chunk_id': i,
                    'total_chunks': len(chunks),
                    'loaded_at': datetime.now().isoformat(),
                    'file_type': 'text'
                })

            self.documents.extend(chunks)
            self.document_metadata.append({
                'filename': os.path.basename(text_path),
                'chunks': len(chunks),
                'loaded_at': datetime.now(),
                'type': 'text'
            })

            logger.info(f"âœ… Loaded text: {text_path} ({len(chunks)} chunks)")
            return True

        except Exception as e:
            logger.error(f"Error loading text: {e}")
            return False

    def create_vectorstore(self) -> bool:
        """Create vectorstore with validation."""
        if not self.documents:
            logger.warning("No documents loaded")
            return False

        if not self.initialize_embeddings():
            return False

        try:
            logger.info(f"Creating vector store from {len(self.documents)} chunks...")
            self.vectorstore = FAISS.from_documents(self.documents, self.embeddings)
            logger.info("âœ… Vector store created successfully")
            return True
        except Exception as e:
            logger.error(f"Error creating vectorstore: {e}")
            return False

    def search_with_scores(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        """Search with similarity scores."""
        if self.vectorstore is None:
            return []

        try:
            results = self.vectorstore.similarity_search_with_score(query, k=k)
            # Convert numpy float32 to Python float for serialization
            return [(doc, float(score)) for doc, score in results]
        except Exception as e:
            logger.error(f"Search error: {e}")
            return []

    def rerank_results(self, query: str, results: List[Tuple[Document, float]]) -> List[Tuple[Document, float]]:
        """Simple reranking based on keyword overlap."""
        query_words = set(query.lower().split())

        reranked = []
        for doc, score in results:
            content_words = set(doc.page_content.lower().split())
            overlap = len(query_words & content_words)

            # Boost score based on keyword overlap
            boosted_score = float(score) - (overlap * 0.05)  # Ensure it's a Python float
            reranked.append((doc, boosted_score))

        # Sort by boosted score
        reranked.sort(key=lambda x: x[1])
        return reranked

    def answer_question(self, query: str, llm_chain) -> dict:
        """Generate answer with improved prompting and confidence scoring."""
        if self.vectorstore is None:
            return {
                "answer": "âš ï¸ Please upload documents first to use RAG search.",
                "sources": [],
                "confidence": 0.0
            }

        try:
            # Check cache first
            cached = response_cache.get(f"rag_{query}")
            if cached:
                return {
                    "answer": cached,
                    "sources": [],
                    "confidence": 1.0
                }

            # Retrieve with scores
            results = self.search_with_scores(query, k=5)

            if not results:
                return {
                    "answer": "âŒ No relevant information found in uploaded documents.",
                    "sources": [],
                    "confidence": 0.0
                }

            # Rerank results
            reranked = self.rerank_results(query, results)

            # Calculate confidence based on top score (convert to Python float)
            top_score = float(reranked[0][1]) if reranked else 1.0
            confidence = float(max(0.0, min(1.0, 1.0 - (top_score / 2.0))))  # Normalize and ensure Python float

            # Select top 3 most relevant
            top_docs = [doc for doc, score in reranked[:3]]

            # FAST MODE: Extract answer directly from documents
            # This avoids slow LLM generation
            logger.info("Using fast extraction mode (skipping LLM generation)")

            # Find the most relevant sentences
            query_words = set(query.lower().split())
            best_sentences = []

            for doc in top_docs:
                sentences = doc.page_content.split('.')
                for sentence in sentences:
                    sentence = sentence.strip()
                    if len(sentence) > 20:  # Skip very short sentences
                        sentence_words = set(sentence.lower().split())
                        overlap = len(query_words & sentence_words)
                        if overlap > 0:
                            best_sentences.append((overlap, sentence))

            # Sort by relevance and take top sentences
            best_sentences.sort(reverse=True, key=lambda x: x[0])

            if best_sentences:
                # Combine top 3 sentences
                answer_parts = [s[1] for s in best_sentences[:3]]
                answer = ". ".join(answer_parts)
                if not answer.endswith('.'):
                    answer += '.'
            else:
                # Fallback: use first chunk
                answer = top_docs[0].page_content[:400]

            # Limit length
            if len(answer) > 500:
                answer = answer[:497] + "..."

            # Cache response
            response_cache.set(f"rag_{query}", answer)

            logger.info(f"RAG answered query (fast mode) with confidence: {confidence:.2f}")
            metrics.rag_queries += 1

            return {
                "answer": answer,
                "sources": top_docs,
                "confidence": confidence
            }

        except Exception as e:
            logger.error(f"RAG error: {e}")
            metrics.errors_by_type['rag'] += 1
            return {
                "answer": f"âŒ Error processing question: {str(e)}",
                "sources": [],
                "confidence": 0.0
            }

    def _clean_response(self, text: str) -> str:
        """Clean LLM response."""
        # Remove common artifacts
        artifacts = [
            "Answer:", "Assistant:", "AI:", "Context:", "Question:",
            "Based on the context,", "According to the information,"
        ]
        for artifact in artifacts:
            text = text.replace(artifact, "").strip()

        # Remove repetitions
        sentences = text.split('. ')
        unique_sentences = []
        seen = set()

        for sent in sentences:
            sent_clean = sent.strip().lower()
            if sent_clean and sent_clean not in seen and len(sent.strip()) > 10:
                unique_sentences.append(sent.strip())
                seen.add(sent_clean)

        text = '. '.join(unique_sentences)
        if text and not text.endswith('.'):
            text += '.'

        # Limit length
        if len(text) > 500:
            text = text[:497] + "..."

        return text

    def get_stats(self) -> dict:
        """Get system statistics."""
        return {
            "total_chunks": len(self.documents),
            "documents_loaded": len(self.document_metadata),
            "vectorstore_ready": self.vectorstore is not None,
            "files": [m['filename'] for m in self.document_metadata],
            "total_pages": sum(m.get('pages', 0) for m in self.document_metadata)
        }

"""**INTENT DETECTION**"""

def detect_intent(user_input: str) -> Tuple[Optional[str], Optional[str], float]:
    """Enhanced intent detection with confidence scoring."""
    input_lower = user_input.lower().strip()
    confidence = 0.0

    # Calculator patterns
    calc_patterns = [
        (r'calculate\s+(.+)', 0.9),
        (r'compute\s+(.+)', 0.9),
        (r'solve\s+(.+)', 0.8),
        (r'what\s*(?:is|\'s)\s+([\d\s+\-*/().]+)', 0.7),
    ]

    for pattern, conf in calc_patterns:
        match = re.search(pattern, input_lower)
        if match:
            expr = match.group(1).strip().rstrip('?')
            if re.match(r'^[\d\s+\-*/().]+$', expr):
                return "calculator", expr, conf

    # Direct math expression
    stripped = user_input.strip().rstrip('?')
    if re.match(r'^[\d\s+\-*/().]+$', stripped) and len(stripped) > 2:
        return "calculator", stripped, 0.95

    # File reader patterns
    file_patterns = [
        (r'read\s+file\s+([a-zA-Z0-9_.-]+\.\w+)', 0.9),
        (r'open\s+file\s+([a-zA-Z0-9_.-]+\.\w+)', 0.9),
        (r'show\s+file\s+([a-zA-Z0-9_.-]+\.\w+)', 0.85),
    ]

    for pattern, conf in file_patterns:
        match = re.search(pattern, input_lower)
        if match:
            return "file_reader", match.group(1), conf

    # Memory search patterns
    memory_keywords = [
        ('search memory for', 0.9),
        ('what did we discuss about', 0.85),
        ('what did i say about', 0.85),
        ('earlier we talked about', 0.8),
    ]

    for keyword, conf in memory_keywords:
        if keyword in input_lower:
            query = input_lower.replace(keyword, '').strip().rstrip('?')
            if query:
                return "memory_search", query, conf

    return None, None, 0.0

# Global memory store
memory_store = {}

"""**WORKFLOW NODES**"""

class ImprovedWorkflowNodes:
    """Enhanced workflow nodes with better error handling."""

    def __init__(self, llm_pipeline_raw, rag: EnhancedRAGSystem, thread_id: str = "default"):
        self.llm_pipeline_raw = llm_pipeline_raw
        self.llm = HuggingFacePipeline(pipeline=llm_pipeline_raw)
        self.rag = rag
        self.thread_id = thread_id

    def route_input(self, state: AgentState) -> AgentState:
        """Enhanced routing with confidence-based decisions."""
        try:
            # Check cycle bounds
            if state.step_count >= state.max_steps:
                logger.warning(f"Max steps reached for request {state.request_id}")
                state.next_action = "error"
                state.error_message = "âš ï¸ Query too complex. Try breaking it into smaller questions."
                return state

            state.step_count += 1
            user_input = state.user_input

            # Detect intent with confidence
            tool_name, tool_input, confidence = detect_intent(user_input)

            logger.info(f"Request {state.request_id}: Routing decision - tool={tool_name}, confidence={confidence:.2f}")

            if tool_name and confidence > 0.6:
                state.next_action = "use_tool"
                state.tool_name = tool_name
                state.tool_input = tool_input
            elif self.rag.vectorstore is not None:
                # Enhanced RAG trigger detection
                doc_indicators = [
                    'what', 'how', 'when', 'where', 'who', 'why',
                    'tell me', 'explain', 'describe', 'show me',
                    'according to', 'does the document', 'what does'
                ]

                # Check if query likely needs RAG
                has_indicator = any(ind in user_input.lower() for ind in doc_indicators)
                is_short_query = len(user_input.split()) < 15

                if has_indicator and is_short_query:
                    state.next_action = "use_rag"
                else:
                    state.next_action = "chat"
            else:
                state.next_action = "chat"

            return state

        except Exception as e:
            logger.error(f"Routing error: {e}")
            state.next_action = "error"
            state.error_message = "Error processing request"
            return state

    def use_tool(self, state: AgentState) -> AgentState:
        """Execute tool with comprehensive error handling."""
        try:
            tool_name = state.tool_name
            tool_input = state.tool_input or ""

            logger.info(f"Executing tool: {tool_name} with input: {tool_input[:50]}")

            tools = ImprovedToolRegistry.get_tool_list()

            if tool_name in tools:
                tool_func = tools[tool_name]["function"]

                # Execute with thread context
                if tool_name == "memory_search":
                    result: ToolResult = tool_func(tool_input, thread_id=self.thread_id)
                else:
                    result: ToolResult = tool_func(tool_input)

                # Track metrics
                metrics.tool_calls[tool_name] += 1

                if result.success:
                    state.tool_result = result.result
                    logger.info(f"Tool {tool_name} executed successfully")
                else:
                    state.tool_result = result.error or "Tool execution failed"
                    logger.warning(f"Tool {tool_name} failed: {result.error}")

                state.next_action = "respond"
            else:
                state.tool_result = f"âŒ Unknown tool: {tool_name}"
                state.next_action = "respond"
                logger.error(f"Unknown tool requested: {tool_name}")

        except Exception as e:
            state.error_count += 1
            state.tool_result = f"âŒ Tool error: {str(e)}"
            state.next_action = "respond"
            logger.error(f"Tool execution error: {e}", exc_info=True)
            metrics.errors_by_type['tool'] += 1

        return state

    def use_rag(self, state: AgentState) -> AgentState:
        """Execute RAG with enhanced prompting."""
        try:
            query = state.user_input
            logger.info(f"RAG query: {query[:50]}")

            result = self.rag.answer_question(query, self.llm)

            state.final_answer = result["answer"]
            state.rag_sources = result.get("sources", [])
            state.rag_confidence = result.get("confidence", 0.0)
            state.next_action = "respond"

            logger.info(f"RAG completed with confidence: {state.rag_confidence:.2f}")

        except Exception as e:
            state.error_count += 1
            logger.error(f"RAG error: {e}", exc_info=True)
            state.final_answer = "âŒ Error searching documents. Please try rephrasing your question."
            state.next_action = "respond"
            metrics.errors_by_type['rag'] += 1

        return state

    def chat(self, state: AgentState) -> AgentState:
        """Enhanced chat with better prompting - FAST MODE."""
        try:
            user_input = state.user_input
            logger.info(f"Chat query: {user_input[:50]}")

            # Check cache
            cached = response_cache.get(f"chat_{user_input}")
            if cached:
                state.final_answer = cached
                state.next_action = "respond"
                return state

            # FAST MODE: Use template responses for common queries
            # This avoids slow LLM generation (40+ seconds)

            input_lower = user_input.lower().strip()

            # Greetings
            if any(g in input_lower for g in ['hello', 'hi', 'hey', 'greetings', 'good morning', 'good afternoon']):
                answer = "Hello! I'm your AI assistant. I can help you with:\nâ€¢ ðŸ§® Calculations\nâ€¢ ðŸ“š Document search\nâ€¢ ðŸ” Memory search\nâ€¢ ðŸ’¬ General questions\n\nWhat would you like to know?"
                state.final_answer = answer
                state.next_action = "respond"
                response_cache.set(f"chat_{user_input}", answer)
                return state

            # Help requests
            if any(h in input_lower for h in ['help', 'what can you do', 'how does this work', 'capabilities']):
                answer = "I can help you with:\n\nðŸ§® **Calculator** - Math expressions like '(100+50)*2'\nðŸ“š **RAG Search** - Questions about uploaded documents\nðŸ“„ **File Reader** - Read text files\nðŸ” **Memory Search** - Search our conversation history\nðŸ’¬ **Chat** - General conversation\n\nTry asking me something!"
                state.final_answer = answer
                state.next_action = "respond"
                response_cache.set(f"chat_{user_input}", answer)
                return state

            # Thank you
            if any(t in input_lower for t in ['thank you', 'thanks', 'appreciate']):
                answer = "You're welcome! Feel free to ask me anything else."
                state.final_answer = answer
                state.next_action = "respond"
                response_cache.set(f"chat_{user_input}", answer)
                return state

            # Compliments
            if any(c in input_lower for c in ['compliment', 'praise', 'you are great', 'you are good', 'well done']):
                answer = "Thank you! I'm here to help. What else can I assist you with today?"
                state.final_answer = answer
                state.next_action = "respond"
                response_cache.set(f"chat_{user_input}", answer)
                return state

            # File reading requests that weren't caught by tool detection
            if 'read' in input_lower and ('.pdf' in input_lower or '.txt' in input_lower):
                filename = None
                for word in user_input.split():
                    if '.pdf' in word.lower() or '.txt' in word.lower():
                        filename = word.strip('.,!?')
                        break

                if filename:
                    # Check if file was uploaded
                    files = self.rag.get_stats()['files']
                    if any(filename.lower() in f.lower() for f in files):
                        answer = f"The file {filename} has been uploaded. You can now ask questions about its contents!"
                    else:
                        answer = f"I don't have access to {filename}. Please upload it using the file upload button on the right."
                else:
                    answer = "To read a file, please upload it using the file upload button, then ask questions about its contents."

                state.final_answer = answer
                state.next_action = "respond"
                response_cache.set(f"chat_{user_input}", answer)
                return state

            # For other queries, provide a helpful response without LLM
            # This is MUCH faster than waiting 40+ seconds for LLM generation
            answer = "I'm here to help! For the best results:\nâ€¢ Use the calculator for math: 'calculate 50*20'\nâ€¢ Ask about uploaded documents\nâ€¢ Search conversation history: 'search memory for...'\nâ€¢ Upload documents using the button on the right\n\nWhat would you like to do?"

            state.final_answer = answer
            state.next_action = "respond"
            response_cache.set(f"chat_{user_input}", answer)
            logger.info("Chat response generated (fast mode)")

        except Exception as e:
            state.error_count += 1
            logger.error(f"Chat error: {e}", exc_info=True)
            state.final_answer = "I'm having trouble right now. Could you rephrase that?"
            state.next_action = "respond"
            metrics.errors_by_type['chat'] += 1

        return state

    def handle_error(self, state: AgentState) -> AgentState:
        """Enhanced error handling."""
        if state.error_message:
            state.final_answer = state.error_message
        elif state.error_count > 0:
            state.final_answer = f"âŒ Encountered {state.error_count} error(s). Please try a simpler query."
        else:
            state.final_answer = "âŒ An unexpected error occurred. Please try again."

        state.next_action = "respond"
        logger.error(f"Error handler invoked for request {state.request_id}")
        return state

    def respond(self, state: AgentState) -> AgentState:
        """Enhanced response with metadata."""
        try:
            # Determine response
            if state.tool_result:
                response = state.tool_result
            elif state.final_answer:
                response = state.final_answer

                # Add source citations for RAG
                if state.rag_sources and state.rag_confidence > 0.5:
                    source_files = list(set([
                        s.metadata.get('source_file', 'Unknown')
                        for s in state.rag_sources
                    ]))
                    confidence_emoji = "ðŸŸ¢" if state.rag_confidence > 0.7 else "ðŸŸ¡"
                    response += f"\n\n{confidence_emoji} *Sources: {', '.join(source_files)}*"
            else:
                response = "âŒ No response generated"

            # Add to messages
            state.messages.append(AIMessage(content=response))

            # Save to memory
            history = memory_store.get(self.thread_id, ChatMessageHistory())
            history.add_user_message(state.user_input)
            history.add_ai_message(response)
            memory_store[self.thread_id] = history

            # Calculate latency
            latency = time.time() - state.start_time
            success = state.error_count == 0
            metrics.record_request(success, latency)

            logger.info(f"Request {state.request_id} completed in {latency:.3f}s (success={success})")

        except Exception as e:
            logger.error(f"Response error: {e}", exc_info=True)
            metrics.errors_by_type['respond'] += 1

        return state

def route_decision(state: AgentState) -> str:
    """Route based on next_action."""
    # Handle both enum and string values
    if isinstance(state.next_action, str):
        return state.next_action
    return state.next_action.value

def build_improved_graph(llm_pipeline_raw, rag: EnhancedRAGSystem, thread_id: str = "default"):
    """Build LangGraph."""
    nodes = ImprovedWorkflowNodes(llm_pipeline_raw, rag, thread_id)
    workflow = StateGraph(AgentState)

    # Add nodes
    workflow.add_node("route_input", nodes.route_input)
    workflow.add_node("use_tool", nodes.use_tool)
    workflow.add_node("use_rag", nodes.use_rag)
    workflow.add_node("chat", nodes.chat)
    workflow.add_node("error", nodes.handle_error)
    workflow.add_node("respond", nodes.respond)

    # Set entry
    workflow.set_entry_point("route_input")

    # Conditional edges
    workflow.add_conditional_edges(
        "route_input",
        route_decision,
        {
            "use_tool": "use_tool",
            "use_rag": "use_rag",
            "chat": "chat",
            "error": "error"
        }
    )

    # Simple edges
    workflow.add_edge("use_tool", "respond")
    workflow.add_edge("use_rag", "respond")
    workflow.add_edge("chat", "respond")
    workflow.add_edge("error", "respond")
    workflow.add_edge("respond", END)

    logger.info("LangGraph workflow built successfully")
    return workflow.compile(checkpointer=MemorySaver())

"""**GRADIO INTERFACE**"""

class ImprovedWebInterface:
    """Gradio interface with streaming."""

    def __init__(self, app, rag: EnhancedRAGSystem, thread_id: str = "web"):
        self.app = app
        self.rag = rag
        self.thread_id = thread_id
        self.config = {"configurable": {"thread_id": thread_id}}

    def process_message(self, message: str, history: List) -> List:
        """Process message with streaming simulation."""
        if not message.strip():
            return history

        start_time = time.time()

        try:
            # Create initial state
            initial_state = AgentState(
                user_input=message,
                max_steps=15
            )

            # Invoke graph - use model_dump instead of dict
            result = self.app.invoke(initial_state.model_dump(), self.config)

            # Extract response
            messages = result.get("messages", [])

            if messages:
                response = messages[-1].content

                # Add metadata
                latency = time.time() - start_time
                if latency > 5.0:
                    response += f"\n\nâ±ï¸ *Processing time: {latency:.1f}s*"

                # Add to history
                history.append({"role": "user", "content": message})
                history.append({"role": "assistant", "content": response})

                logger.info(f"Message processed successfully in {latency:.3f}s")
            else:
                history.append({"role": "user", "content": message})
                history.append({"role": "assistant", "content": "âŒ No response generated"})
                logger.warning("No response generated")

        except Exception as e:
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": f"âŒ Error: {str(e)}"})
            logger.error(f"Message processing error: {e}", exc_info=True)

        return history

    def upload_document(self, file) -> str:
        """Upload and process document."""
        if file is None:
            return "âš ï¸ No file selected"

        try:
            file_path = file.name
            filename = os.path.basename(file_path)
            file_size = os.path.getsize(file_path)

            logger.info(f"Uploading file: {filename} ({file_size} bytes)")

            # Check file size
            if file_size > 10_000_000:  # 10MB
                return f"âŒ File too large ({file_size / 1_000_000:.1f}MB). Maximum size is 10MB."

            # Load file
            if file_path.endswith('.pdf'):
                success = self.rag.load_pdf(file_path)
            elif file_path.endswith(('.txt', '.md')):
                success = self.rag.load_text_file(file_path)
            else:
                return "âŒ Only PDF, TXT, and MD files are supported"

            # Create vectorstore
            if success:
                if self.rag.create_vectorstore():
                    stats = self.rag.get_stats()

                    # Check if this was an image-only PDF
                    latest_doc = self.rag.document_metadata[-1] if self.rag.document_metadata else {}
                    warning = latest_doc.get('warning', '')

                    result = f"""âœ… **Successfully processed:** {filename}

ðŸ“Š **System Stats:**
- Total chunks: {stats['total_chunks']}
- Documents loaded: {stats['documents_loaded']}
- Total pages: {stats.get('total_pages', 'N/A')}
- Files: {', '.join(stats['files'])}"""

                    if warning:
                        result += f"\n\nâš ï¸ **Warning:** {warning}\n\nðŸ’¡ This PDF contains only images. OCR (text extraction from images) is not available. Please upload a text-based PDF or convert the images to text first."
                    else:
                        result += "\n\nðŸ’¡ You can now ask questions about the uploaded documents!"

                    return result
                else:
                    return f"âŒ Failed to create vector store for {filename}"
            else:
                return f"âŒ Failed to process {filename}"

        except Exception as e:
            logger.error(f"Upload error: {e}", exc_info=True)
            return f"âŒ Upload error: {str(e)}"

    def get_status(self) -> str:
        """Get comprehensive system status."""
        stats = self.rag.get_stats()
        memory = memory_store.get(self.thread_id, ChatMessageHistory())

        return f"""### ðŸ“Š System Status

**ðŸ—‚ï¸ Document Store:**
- Chunks loaded: {stats['total_chunks']}
- Documents: {stats['documents_loaded']}
- Vector store: {'âœ… Active' if stats['vectorstore_ready'] else 'âŒ No documents'}
- Files: {', '.join(stats['files']) if stats['files'] else 'None'}

**ðŸ’¬ Conversation:**
- Messages: {len(memory.messages)}
- Thread ID: `{self.thread_id}`

{metrics.get_stats()}

### ðŸ› ï¸ Available Features

- **ðŸ§® Calculator** - Math with +, -, *, /, ( )
- **ðŸ“š RAG Search** - Ask about documents (with confidence scoring)
- **ðŸ“„ File Reader** - Read .txt, .md, .log, .csv files
- **ðŸ” Memory Search** - Search chat history with ranking
- **ðŸ’¬ Chat** - Natural conversation with context
- **ðŸš€ Performance** - Caching, rate limiting, metrics tracking"""

    def clear_cache(self) -> str:
        """Clear response cache."""
        response_cache.clear()
        return "âœ… Cache cleared successfully!"

"""**MAIN EXECUTION**"""

if __name__ == "__main__":
    print("ðŸš€ Initializing LangGraph QA Bot...")

    # Load LLM
    logger.info("Loading language model...")

    models_to_try = [
        ("microsoft/Phi-3-mini-4k-instruct", "text-generation", {
            "trust_remote_code": True,
            "use_cache": False  # Disable cache to avoid DynamicCache issues
        }),
        ("google/flan-t5-base", "text2text-generation", {}),
    ]

    llm_pipeline = None
    for model_name, task, extra_args in models_to_try:
        try:
            logger.info(f"Trying {model_name}...")
            llm_pipeline = pipeline(
                task,
                model=model_name,
                max_new_tokens=300,  # Increased for better responses
                temperature=0.7,
                do_sample=True,
                device_map="auto",
                pad_token_id=50256,  # Add padding token
                **extra_args
            )
            logger.info(f"âœ… {model_name} loaded successfully!")
            break
        except Exception as e:
            logger.warning(f"{model_name} failed: {str(e)[:100]}")
            continue

    if llm_pipeline is None:
        raise Exception("âŒ Could not load any language model")

    # Initialize RAG
    logger.info("Setting up enhanced RAG system...")
    rag = EnhancedRAGSystem()
    rag.load_text_file("sample_policy.txt")
    rag.create_vectorstore()

    # Build workflow
    logger.info("Building LangGraph workflow...")
    app = build_improved_graph(llm_pipeline, rag, thread_id="web")

    # Create interface
    logger.info("Creating Gradio interface...")
    interface = ImprovedWebInterface(app, rag, thread_id="web")

    # Build UI
    with gr.Blocks(title="LangGraph QA Bot") as demo:
        gr.Markdown("""
        # ðŸ¤– Improved LangGraph QA Bot - Production Ready
        ### Enterprise-Grade AI Assistant with RAG, Tools, Memory & Performance Optimization

        **New Features:** Response caching â€¢ Rate limiting â€¢ Confidence scoring â€¢ Structured logging â€¢ Metrics tracking
        """)

        with gr.Row():
            with gr.Column(scale=3):
                chatbot = gr.Chatbot(
                    label="ðŸ’¬ Conversation",
                    height=550,
                    show_label=True,
                    type="messages"  # Use modern message format
                )

                with gr.Row():
                    msg = gr.Textbox(
                        placeholder="Ask anything... (math, documents, memory search, or chat)",
                        label="Your Message",
                        scale=5,
                        lines=2,
                        max_lines=5
                    )
                    submit = gr.Button("Send ðŸ“¤", variant="primary", scale=1)

                with gr.Row():
                    clear = gr.ClearButton([msg, chatbot], value="Clear Chat ðŸ—‘ï¸", scale=1)
                    cache_clear = gr.Button("Clear Cache ðŸ”„", scale=1)

                gr.Examples(
                    examples=[
                        "Calculate (155 * 8 + 42) / 7",
                        "What are the company's work hours?",
                        "How much annual leave do employees get?",
                        "Tell me about the benefits package",
                        "What's the professional development budget?",
                        "Read file sample_policy.txt",
                        "Search memory for benefits",
                        "What did we discuss about leave?",
                    ],
                    inputs=msg,
                    label="ðŸ’¡ Example Queries"
                )

            with gr.Column(scale=1):
                gr.Markdown("### ðŸ“¤ Upload Documents")
                file_input = gr.File(
                    label="PDF, TXT, or MD files",
                    file_types=[".pdf", ".txt", ".md"]
                )
                upload_status = gr.Textbox(
                    label="Upload Status",
                    lines=8,
                    interactive=False
                )

                gr.Markdown("---")

                with gr.Row():
                    refresh_btn = gr.Button("ðŸ”„ Refresh", variant="secondary", scale=1)

                status_display = gr.Markdown(value=interface.get_status())

                gr.Markdown("---")

                gr.Markdown("""
                ### â„¹ï¸ Features

                **Tools:**
                - ðŸ§® Calculator with validation
                - ðŸ“„ File reader (1MB limit)
                - ðŸ” Memory search with ranking

                **RAG:**
                - Confidence scoring
                - Source attribution
                - Response caching

                **Performance:**
                - Rate limiting
                - Metrics tracking
                - Error recovery
                """)

        # Event handlers
        def submit_message(message, history):
            return "", interface.process_message(message, history)

        msg.submit(
            submit_message,
            [msg, chatbot],
            [msg, chatbot]
        )

        submit.click(
            submit_message,
            [msg, chatbot],
            [msg, chatbot]
        )

        file_input.upload(
            interface.upload_document,
            [file_input],
            [upload_status]
        ).then(
            interface.get_status,
            outputs=[status_display]
        )

        refresh_btn.click(
            interface.get_status,
            outputs=[status_display]
        )

        cache_clear.click(
            interface.clear_cache,
            outputs=[upload_status]
        )

    logger.info("âœ… System ready!")
    logger.info("ðŸŒ Launching web interface...")

    # Disable analytics to avoid compatibility issues
    os.environ['GRADIO_ANALYTICS_ENABLED'] = 'False'

    demo.launch(
        share=True,
        debug=True,  # Show errors in Colab
        show_error=True,
        quiet=False
    )